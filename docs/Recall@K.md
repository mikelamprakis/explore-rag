# 1ï¸âƒ£ What Recall@K means

**Recall@K** measures **coverage**.

> It answers:
> **â€œDid the retriever manage to fetch *all the relevant documents* within the top K results?â€**

It does **not care about ranking order**, only whether relevant items appear *anywhere* in the top K.

---

# 2ï¸âƒ£ Formal definition

For a single query:

```
Recall@K =
(number of relevant documents retrieved in the top K results)
----------------------------------------------------------------
(total number of relevant documents for that query)

```

Overall Recall@K is the **average across queries**.

---

# 3ï¸âƒ£ Simple RAG example

Assume for a query there are **4 relevant chunks** in the corpus.

### Retrieved top-5 chunks:

* 2 are relevant
* 3 are irrelevant

![img_6.png](img/img_6.png)

If all 4 relevant chunks appear somewhere in top-5:

![img_7.png](img/img_7.png)

---

# 4ï¸âƒ£ Why Recall@K is critical in RAG

RAG systems **cannot generate correct answers** if the required information is **never retrieved**.

Recall@K answers:

> **â€œDid we even give the LLM a chance?â€**

Low recall means:

* Hallucination is guaranteed
* Prompt tuning wonâ€™t help
* Model size doesnâ€™t matter

---

# 5ï¸âƒ£ Recall@K ignores rank â€” on purpose

This is intentional.

| Metric   | Cares about rank? |
| -------- | ----------------- |
| MRR      | âœ… Very much       |
| nDCG     | âœ… Gradually       |
| Recall@K | âŒ Not at all      |

You can retrieve relevant chunks at ranks **2, 5, and 9**, and Recall@10 will still be **1.0**.

---

# 6ï¸âƒ£ Typical K values in RAG

Common choices:

| K         | Use case              |
| --------- | --------------------- |
| Recall@3  | Tight context windows |
| Recall@5  | Standard QA           |
| Recall@10 | Long-context models   |
| Recall@20 | Multi-hop reasoning   |

**Choose K to match your context budget**, not arbitrarily.

---

# 7ï¸âƒ£ Binary vs multi-relevance recall

### Binary Recall (most common)

* Relevant = yes/no
* Simple and fast

### Graded Recall (less common)

* Count only relevance â‰¥ threshold
* Useful when labeling is nuanced

In RAG, **binary recall** is usually enough.

---

# 8ï¸âƒ£ Common Recall@K failure modes

### âŒ Low Recall

* Poor embeddings for domain
* Overly aggressive chunking
* Queries are underspecified
* Vector-only search (no BM25)
* Index filtering mistakes

### âŒ High Recall but bad answers

* Too many irrelevant chunks
* No reranking
* Context truncation
* Prompt ignores lower-ranked docs

---

# 9ï¸âƒ£ Recall@K vs Precision@K (important contrast)

| Metric      | Focus                          |
| ----------- | ------------------------------ |
| Recall@K    | â€œDid I get *everything*?â€      |
| Precision@K | â€œDid I get *only good stuff*?â€ |

In RAG:

* **Recall > Precision**
* Noise can be filtered
* Missing info cannot be recovered

---

# ğŸ”Ÿ How Recall@K fits with MRR and nDCG

Think of retrieval quality as **three gates**:

1ï¸âƒ£ **Recall@K** â€“ Did we retrieve the needed info at all?
2ï¸âƒ£ **MRR** â€“ Did we retrieve it early enough?
3ï¸âƒ£ **nDCG** â€“ Did we order all useful info well?

| Scenario              | Recall | MRR | nDCG | Outcome        |
| --------------------- | ------ | --- | ---- | -------------- |
| Low recall            | âŒ      | âŒ   | âŒ    | Hallucinations |
| High recall, low MRR  | âœ…      | âŒ   | âš ï¸   | Weak answers   |
| High recall, high MRR | âœ…      | âœ…   | âš ï¸   | OK answers     |
| All high              | âœ…      | âœ…   | âœ…    | Strong RAG     |

---

# 1ï¸âƒ£1ï¸âƒ£ Typical target ranges (rough)

For production RAG:

* **Recall@5 â‰¥ 0.8**
* **Recall@10 â‰¥ 0.9**

If Recall@K is low:

> Stop tuning the LLM â€” fix retrieval.

---

# ğŸ”‘ Final mental model

Think of Recall@K as:

> **â€œDid I bring the right books into the room?â€**

MRR asks:

> â€œIs the right book already open on the desk?â€

nDCG asks:

> â€œAre the books arranged in a useful order?â€

